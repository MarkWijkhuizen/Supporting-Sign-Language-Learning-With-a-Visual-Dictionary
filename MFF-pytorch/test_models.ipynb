{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09c55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from dataset import TSNDataSet\n",
    "from models import TSN\n",
    "from transforms import *\n",
    "from ops import ConsensusModule\n",
    "from tqdm.notebook import tqdm\n",
    "from test_opts import parser\n",
    "\n",
    "import datasets_video\n",
    "import pdb\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b79ee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'jester',\n",
       " 'modality': 'RGBFlow',\n",
       " 'weights': 'pretrained_models/MFF_jester_RGBFlow_BNInception_segment4_3f1c_best.pth.tar',\n",
       " 'arch': 'BNInception',\n",
       " 'save_scores': None,\n",
       " 'test_segments': 4,\n",
       " 'max_num': -1,\n",
       " 'test_crops': 1,\n",
       " 'input_size': 224,\n",
       " 'num_motion': 3,\n",
       " 'consensus_type': 'MLP',\n",
       " 'workers': 0,\n",
       " 'gpus': None,\n",
       " 'img_feature_dim': 256,\n",
       " 'num_set_segments': 1,\n",
       " 'softmax': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Options\n",
    "display(easydict.EasyDict(vars(parser.parse_known_args()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0934297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e5465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_video(video_data):\n",
    "        i, data, label = video_data\n",
    "        num_crop = args.test_crops\n",
    "\n",
    "        if args.modality == 'RGB':\n",
    "            length = 3\n",
    "        elif args.modality == 'Flow':\n",
    "            length = 10\n",
    "        elif args.modality == 'RGBDiff':\n",
    "            length = 18\n",
    "        elif args.modality == 'RGBFlow':\n",
    "            length = 3 + 2 * args.num_motion # 3 rgb channels and 3*2=6 flow channels \n",
    "        else:\n",
    "            raise ValueError(\"Unknown modality \"+args.modality)\n",
    "\n",
    "        input_var = torch.autograd.Variable(data.view(-1, length, data.size(2), data.size(3)),\n",
    "                                            volatile=True)\n",
    "        rst = net(input_var)\n",
    "        if args.softmax==1:\n",
    "            # take the softmax to normalize the output to probability\n",
    "            rst = F.softmax(rst)\n",
    "\n",
    "        rst = rst.data.cpu().numpy().copy()\n",
    "\n",
    "        if args.consensus_type in ['MLP']:\n",
    "            rst = rst.reshape(-1, 1, num_class)\n",
    "        else:\n",
    "            rst = rst.reshape((num_crop, args.test_segments, num_class)).mean(axis=0).reshape((args.test_segments, 1, num_class))\n",
    "\n",
    "        return i, rst, label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360c82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, net, num_class\n",
    "    args = easydict.EasyDict(vars(parser.parse_known_args()[0]))\n",
    "    args.batch_size = 1\n",
    "    \n",
    "    categories, args.train_list, args.val_list, args.root_path, prefix = datasets_video.return_dataset(args.dataset, args.modality)\n",
    "    num_class = len(categories)\n",
    "    \n",
    "    net = TSN(num_class, args.test_segments if args.consensus_type in ['MLP'] else 1, args.modality,\n",
    "              base_model=args.arch,\n",
    "              consensus_type=args.consensus_type,\n",
    "              img_feature_dim=args.img_feature_dim,\n",
    "              )\n",
    "\n",
    "    checkpoint = torch.load(args.weights)\n",
    "    print('model epoch %d best prec@1: %.2f' % (checkpoint['epoch'], checkpoint['best_prec1']))\n",
    "\n",
    "    base_dict = {'.'.join(k.split('.')[1:]): v for k,v in list(checkpoint['state_dict'].items())}\n",
    "    net.load_state_dict(base_dict)\n",
    "\n",
    "    print(f'Number of test crops: {args.test_crops}')\n",
    "    if args.test_crops == 1:\n",
    "        cropping = torchvision.transforms.Compose([\n",
    "            GroupScale(net.scale_size),\n",
    "            GroupCenterCrop(net.input_size),\n",
    "        ])\n",
    "    elif args.test_crops > 1:\n",
    "        cropping = torchvision.transforms.Compose([\n",
    "            GroupOverSample(net.input_size, net.scale_size)\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Only 1 and 10 crops are supported while we got {}\".format(args.test_crops))\n",
    "\n",
    "    ############ Data Loading Part #####\n",
    "    if args.modality == 'RGB':\n",
    "        data_length = 1\n",
    "    elif args.modality in ['Flow', 'RGBDiff']:\n",
    "        data_length = 5\n",
    "    elif args.modality == 'RGBFlow':\n",
    "        data_length = args.num_motion\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "            TSNDataSet(args.root_path, args.val_list, num_segments=args.test_segments,\n",
    "                       new_length=data_length,\n",
    "                       modality=args.modality,\n",
    "                       image_tmpl=prefix,\n",
    "                       dataset=args.dataset,\n",
    "                       test_mode=True,\n",
    "                       dataset_type='val',\n",
    "                       transform=torchvision.transforms.Compose([\n",
    "                           cropping,\n",
    "                           Stack(roll=(args.arch in\n",
    "                                       ['BNInception','InceptionV3']), isRGBFlow=(args.modality == 'RGBFlow')),\n",
    "                           ToTorchFormatTensor(div=(args.arch not in ['BNInception','InceptionV3'])),\n",
    "                           GroupNormalize(net.input_mean, net.input_std),\n",
    "                       ])),\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=args.workers,\n",
    "                pin_memory=False\n",
    "            )\n",
    "\n",
    "    if args.gpus is not None:\n",
    "        devices = [args.gpus[i] for i in range(args.workers)]\n",
    "    else:\n",
    "        devices = list(range(args.workers))\n",
    "\n",
    "\n",
    "    #net = torch.nn.DataParallel(net.cuda(devices[0]), device_ids=devices)\n",
    "    net = torch.nn.DataParallel(net.cuda())\n",
    "    net.eval()\n",
    "\n",
    "    total_num = len(data_loader.dataset)\n",
    "    output = []\n",
    "\n",
    "    proc_start_time = time.time()\n",
    "    max_num = args.max_num if args.max_num > 0 else len(data_loader.dataset)\n",
    "\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    for i, (data, label) in enumerate(tqdm(data_loader)):\n",
    "        if i >= max_num:\n",
    "            break\n",
    "        rst = eval_video((i, data, label))\n",
    "        output.append(rst[1:])\n",
    "        cnt_time = time.time() - proc_start_time\n",
    "        prec1, prec5 = accuracy(torch.from_numpy(np.mean(rst[1], axis=0)), label, topk=(1, 5))\n",
    "        top1.update(prec1, 1)\n",
    "        top5.update(prec5, 1)\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print('video {} done, total {}/{}, average {:.3f} sec/video, moving Prec@1 {:.3f} Prec@5 {:.3f}'.format(i+1, i+1,\n",
    "                                                                        total_num,\n",
    "                                                                        float(cnt_time) / (i+1), top1.avg, top5.avg))\n",
    "\n",
    "    video_pred = [np.argmax(np.mean(x[0], axis=0)) for x in output]\n",
    "\n",
    "    video_labels = [x[1] for x in output]\n",
    "\n",
    "\n",
    "    cf = confusion_matrix(video_labels, video_pred).astype(float)\n",
    "\n",
    "    cls_cnt = cf.sum(axis=1)\n",
    "    cls_hit = np.diag(cf)\n",
    "\n",
    "    cls_acc = cls_hit / cls_cnt\n",
    "\n",
    "    print('-----Evaluation is finished------')\n",
    "    print('Class Accuracy {:.02f}%'.format(np.mean(cls_acc) * 100))\n",
    "    print('Overall Prec@1 {:.02f}% Prec@5 {:.02f}%'.format(top1.avg, top5.avg))\n",
    "\n",
    "    if args.save_scores is not None:\n",
    "\n",
    "        # reorder before saving\n",
    "        name_list = [x.strip().split()[0] for x in open(args.val_list)]\n",
    "        order_dict = {e:i for i, e in enumerate(sorted(name_list))}\n",
    "        reorder_output = [None] * len(output)\n",
    "        reorder_label = [None] * len(output)\n",
    "        reorder_pred = [None] * len(output)\n",
    "        output_csv = []\n",
    "        for i in range(len(output)):\n",
    "            idx = order_dict[name_list[i]]\n",
    "            reorder_output[idx] = output[i]\n",
    "            reorder_label[idx] = video_labels[i]\n",
    "            reorder_pred[idx] = video_pred[i]\n",
    "            output_csv.append('%s;%s'%(name_list[i], categories[video_pred[i]]))\n",
    "\n",
    "        np.savez(args.save_scores, scores=reorder_output, labels=reorder_label, predictions=reorder_pred, cf=cf)\n",
    "\n",
    "        with open(args.save_scores.replace('npz','csv'),'w') as f:\n",
    "            f.write('\\n'.join(output_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a42a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Initializing TSN with base model: BNInception.\n",
      "    TSN Configurations:\n",
      "        input_modality:     RGBFlow\n",
      "        num_segments:       4\n",
      "        new_length:         3\n",
      "        consensus_module:   MLP\n",
      "        dropout_ratio:      0.8\n",
      "        img_feature_dim:    256\n",
      "            \n",
      "Converting the ImageNet model to RGB+Flow init model\n",
      "Done. RGBFlow model ready.\n",
      "model epoch 38 best prec@1: 92.18\n",
      "Number of test crops: 1\n",
      "Found 14786 val videos\n",
      "Freezing BatchNorm2D except the first one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b33aeb6675f46c380259860a009ee95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1000 done, total 1000/14786, average 0.142 sec/video, moving Prec@1 89.200 Prec@5 99.000\n",
      "video 2000 done, total 2000/14786, average 0.142 sec/video, moving Prec@1 89.350 Prec@5 98.950\n",
      "video 3000 done, total 3000/14786, average 0.144 sec/video, moving Prec@1 89.733 Prec@5 98.900\n",
      "video 4000 done, total 4000/14786, average 0.145 sec/video, moving Prec@1 90.050 Prec@5 99.075\n",
      "video 5000 done, total 5000/14786, average 0.142 sec/video, moving Prec@1 90.420 Prec@5 98.980\n",
      "video 6000 done, total 6000/14786, average 0.140 sec/video, moving Prec@1 90.217 Prec@5 98.900\n",
      "video 7000 done, total 7000/14786, average 0.138 sec/video, moving Prec@1 90.229 Prec@5 98.843\n",
      "video 8000 done, total 8000/14786, average 0.136 sec/video, moving Prec@1 90.062 Prec@5 98.838\n",
      "video 9000 done, total 9000/14786, average 0.134 sec/video, moving Prec@1 90.022 Prec@5 98.833\n",
      "video 10000 done, total 10000/14786, average 0.133 sec/video, moving Prec@1 89.990 Prec@5 98.850\n",
      "video 11000 done, total 11000/14786, average 0.132 sec/video, moving Prec@1 90.045 Prec@5 98.900\n",
      "video 12000 done, total 12000/14786, average 0.132 sec/video, moving Prec@1 90.108 Prec@5 98.925\n",
      "video 13000 done, total 13000/14786, average 0.131 sec/video, moving Prec@1 90.177 Prec@5 98.923\n",
      "video 14000 done, total 14000/14786, average 0.130 sec/video, moving Prec@1 90.214 Prec@5 98.943\n",
      "-----Evaluation is finished------\n",
      "Class Accuracy 89.57%\n",
      "Overall Prec@1 90.26% Prec@5 98.90%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
